#cca175
sqoop import \
    --connect "jdbc:mysql://ms.itversity.com:3306/retail_db" \
    --username retail_user \
    --password itversity \
    --table products \
    --target-dir /user/selvamsand/cloudera/products \
    --as-textfile \
    --fields-terminated-by '|'

hadoop fs -ls  /user/selvamsand/cloudera/products   
hadoop fs -cat  /user/selvamsand/cloudera/products/part-m-00000 | tail

hadoop fs -mkdir /user/selvamsand/cloudera/progblem2
hadoop fs -mv /user/selvamsand/cloudera/products/* \
/user/selvamsand/cloudera/progblem2/products 

hadoop fs -ls /user/selvamsand/cloudera/progblem2/products 

hadoop fs -chmod 765 /user/selvamsand/cloudera/progblem2/products/*

products = sc. \
textFile("/user/selvamsand/cloudera/progblem2/products"). \
map(lambda p : (
                int(p.split('|')[0]), 
                int(p.split('|')[1]),
                p.split('|')[2],
                float(p.split('|')[4]),
                p.split('|')[5]
                )
)      
for i in products.take(10): print(i)

from pyspark.sql import Row
productsDF =  products. \
map(
    lambda p: Row(
                  product_id=p[0],
                  product_cat_id=p[1],
                  product_name=p[2],
                  product_price=p[3],
                  product_image=p[4]
                  )
    ).toDF()

productsDF.show()


from pyspark.sql.functions import *

data_frame_result = productsDF. \
filter("product_price < 100").\
groupby("product_cat_id"). \
agg(
    max("product_price").alias("max_price"),
    countDistinct("product_id").alias("total_products"),
    round(avg("product_price"),2).alias("avg_price"),
    min("product_price").alias("min_price")
    ). \
orderBy("product_cat_id", ascending=False)    
    
data_frame_result.show()

pyspark --master yarn  \
--conf spark.ui.port=12569 \
--packages com.databricks:spark-avro_2.11:4.0.0

sqlContext.setConf("spark.sql.avro.compression.codec","snappy")

data_frame_result.write. \
format("com.databricks.spark.avro"). \
save("/user/selvamsand/cloudera/progblem2/products/result_df")

hadoop fs -ls /user/selvamsand/cloudera/progblem2/products/result_df

productsDF.registerTempTable("products")

sqlContext. \
sql("select * from products limit 10"). \
show()


sql_result = sqlContext. \
sql(
    "select product_cat_id, \
     max(product_price) as max_price, \
     count(distinct(product_id)) as tot_product, \
     cast(avg(product_price) as decimal(10,2)) as avg_price, \
     min(product_price) as min_price \
     from products \
     where product_price < 100 \
     group by product_cat_id \
     order by product_cat_id desc"
     )

sql_result. \
show()

sql_result.write. \
format("com.databricks.spark.avro"). \
save("/user/selvamsand/cloudera/progblem2/products/result_sql")


rdd_result = products. \
filter(lambda p : p[3] < 100). \
map(lambda p : (p[1], p[3])). \
aggregateByKey((0.0, 0.0, 0, 999999999.0),  
            lambda x, y: (max(x[0], y), x[1]+y, x[2]+1, min(x[3], y)),
            lambda x, y: (max(x[0], y[0]), x[1]+y[1], x[2]+y[2], min(x[3], y[3]))
            ). \
sortByKey(False). \
map(lambda p: (p[0], p[1][0], round(p[1][1]/p[1][2], 2), p[1][2], p[1][3]))            

for i in rdd_result.take(10):print(i)

rdd_result. \
map(lambda p: Row(
                  product_cat_id=p[0],
                  max_price=p[1],
                  avg_price=p[2],
                  tot_product=p[3],
                  min_price=p[4]
               )
    ).toDF().show()